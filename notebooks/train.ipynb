{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Vietnamese Hate Speech Detection - Training Notebook\n",
                "\n",
                "This notebook demonstrates how to train a PhoBERT model for hate speech detection on Vietnamese datasets.\n",
                "\n",
                "**Supported Datasets:**\n",
                "- ViHSD (Vietnamese Hate Speech Detection)\n",
                "- ViCTSD (Vietnamese Constructive and Toxic Speech Detection)\n",
                "- ViHOS (Vietnamese Hate and Offensive Spans)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (if needed)\n",
                "# !pip install -r ../requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('../src')\n",
                "\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from huggingface_hub import login\n",
                "\n",
                "# Load environment variables\n",
                "load_dotenv()\n",
                "\n",
                "# Login to Hugging Face\n",
                "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"your_token_here\")\n",
                "login(token=HF_TOKEN)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Dataset\n",
                "\n",
                "Choose one of the three datasets: ViHSD, ViCTSD, or ViHOS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from data_loader import load_dataset_by_name\n",
                "\n",
                "# Choose dataset: \"ViHSD\", \"ViCTSD\", or \"ViHOS\"\n",
                "DATASET_NAME = \"ViHSD\"\n",
                "\n",
                "print(f\"Loading {DATASET_NAME} dataset...\")\n",
                "train_df, val_df, test_df, metadata = load_dataset_by_name(DATASET_NAME)\n",
                "\n",
                "print(f\"\\nDataset: {metadata['name']}\")\n",
                "print(f\"Text column: {metadata['text_col']}\")\n",
                "print(f\"Label column: {metadata['label_col']}\")\n",
                "print(f\"Number of labels: {metadata['num_labels']}\")\n",
                "print(f\"\\nSplit sizes:\")\n",
                "print(f\"  Train: {len(train_df)}\")\n",
                "print(f\"  Val: {len(val_df)}\")\n",
                "print(f\"  Test: {len(test_df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Exploratory Data Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from collections import Counter\n",
                "\n",
                "# Display sample data\n",
                "print(\"Sample data:\")\n",
                "display(train_df.head())\n",
                "\n",
                "# Label distribution\n",
                "all_labels = list(train_df[metadata['label_col']]) + \\\n",
                "             list(val_df[metadata['label_col']]) + \\\n",
                "             list(test_df[metadata['label_col']])\n",
                "\n",
                "label_counts = Counter(all_labels)\n",
                "print(f\"\\nLabel distribution:\")\n",
                "print(dict(sorted(label_counts.items())))\n",
                "\n",
                "# Text length statistics\n",
                "all_lengths = [len(str(text)) for text in train_df[metadata['text_col']]] + \\\n",
                "              [len(str(text)) for text in val_df[metadata['text_col']]] + \\\n",
                "              [len(str(text)) for text in test_df[metadata['text_col']]]\n",
                "\n",
                "print(f\"\\nText length statistics:\")\n",
                "print(f\"  30th percentile: {sorted(all_lengths)[int(len(all_lengths)*0.3)]}\")\n",
                "print(f\"  60th percentile: {sorted(all_lengths)[int(len(all_lengths)*0.6)]}\")\n",
                "print(f\"  95th percentile: {sorted(all_lengths)[int(len(all_lengths)*0.95)]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Configure Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from config import TrainConfig\n",
                "\n",
                "config = TrainConfig(\n",
                "    dataset_name=DATASET_NAME,\n",
                "    model_name=\"vinai/phobert-base\",\n",
                "    max_length=256,\n",
                "    batch_size=16,\n",
                "    epochs=10,\n",
                "    learning_rate=2e-5,\n",
                "    weight_decay=0.01,\n",
                "    warmup_ratio=0.1,\n",
                "    patience=3,\n",
                "    seed=42,\n",
                ")\n",
                "\n",
                "print(\"Training Configuration:\")\n",
                "for key, value in config.to_dict().items():\n",
                "    print(f\"  {key}: {value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Build Model and Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from model import build_model\n",
                "from data_loader import build_torch_dataset\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "# Build model\n",
                "model, tokenizer = build_model(\n",
                "    config.model_name,\n",
                "    metadata['num_labels'],\n",
                "    config.device\n",
                ")\n",
                "\n",
                "print(f\"Model loaded on {config.device}\")\n",
                "\n",
                "# Build datasets\n",
                "train_dataset = build_torch_dataset(\n",
                "    train_df, metadata['text_col'], metadata['label_col'],\n",
                "    tokenizer, config.max_length\n",
                ")\n",
                "val_dataset = build_torch_dataset(\n",
                "    val_df, metadata['text_col'], metadata['label_col'],\n",
                "    tokenizer, config.max_length\n",
                ")\n",
                "test_dataset = build_torch_dataset(\n",
                "    test_df, metadata['text_col'], metadata['label_col'],\n",
                "    tokenizer, config.max_length\n",
                ")\n",
                "\n",
                "# Build dataloaders\n",
                "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=config.batch_size)\n",
                "test_loader = DataLoader(test_dataset, batch_size=config.batch_size)\n",
                "\n",
                "print(f\"Datasets ready: train={len(train_dataset)}, val={len(val_dataset)}, test={len(test_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Setup Optimizer and Scheduler"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import get_cosine_schedule_with_warmup\n",
                "\n",
                "optimizer = torch.optim.AdamW(\n",
                "    model.parameters(),\n",
                "    lr=config.learning_rate,\n",
                "    weight_decay=config.weight_decay,\n",
                ")\n",
                "\n",
                "num_training_steps = len(train_loader) * config.epochs\n",
                "num_warmup_steps = int(num_training_steps * config.warmup_ratio)\n",
                "\n",
                "scheduler = get_cosine_schedule_with_warmup(\n",
                "    optimizer,\n",
                "    num_warmup_steps=num_warmup_steps,\n",
                "    num_training_steps=num_training_steps,\n",
                ")\n",
                "\n",
                "print(f\"Total training steps: {num_training_steps}\")\n",
                "print(f\"Warmup steps: {num_warmup_steps}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
                "from utils import train_epoch, evaluate\n",
                "\n",
                "best_val_f1 = 0.0\n",
                "patience_counter = 0\n",
                "history = {\n",
                "    \"train_loss\": [],\n",
                "    \"val_loss\": [],\n",
                "    \"val_acc\": [],\n",
                "    \"val_f1\": [],\n",
                "    \"epoch_seconds\": [],\n",
                "}\n",
                "\n",
                "print(f\"Starting training on {config.device}...\\n\")\n",
                "training_start = time.time()\n",
                "\n",
                "for epoch in range(1, config.epochs + 1):\n",
                "    epoch_start = time.time()\n",
                "    \n",
                "    # Train\n",
                "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, config.device)\n",
                "    \n",
                "    # Validate\n",
                "    val_preds, val_labels, val_loss = evaluate(model, val_loader, config.device)\n",
                "    val_acc = accuracy_score(val_labels, val_preds)\n",
                "    val_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n",
                "    \n",
                "    epoch_time = time.time() - epoch_start\n",
                "    \n",
                "    # Update history\n",
                "    history[\"train_loss\"].append(train_loss)\n",
                "    history[\"val_loss\"].append(val_loss)\n",
                "    history[\"val_acc\"].append(val_acc)\n",
                "    history[\"val_f1\"].append(val_f1)\n",
                "    history[\"epoch_seconds\"].append(epoch_time)\n",
                "    \n",
                "    # Print summary\n",
                "    print(f\"Epoch {epoch}/{config.epochs} | Time: {epoch_time:.2f}s\")\n",
                "    print(f\"  Train loss: {train_loss:.4f}\")\n",
                "    print(f\"  Val loss: {val_loss:.4f} | Val acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
                "    \n",
                "    # Early stopping\n",
                "    if val_f1 > best_val_f1:\n",
                "        best_val_f1 = val_f1\n",
                "        patience_counter = 0\n",
                "        print(f\"  âœ“ New best F1: {best_val_f1:.4f}. Saving model...\")\n",
                "        model.save_pretrained(config.output_dir)\n",
                "        tokenizer.save_pretrained(config.output_dir)\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        print(f\"  No improvement ({patience_counter}/{config.patience})\")\n",
                "        if patience_counter >= config.patience:\n",
                "            print(\"  Early stopping triggered.\")\n",
                "            break\n",
                "    print()\n",
                "\n",
                "training_time = time.time() - training_start\n",
                "print(f\"Training finished in {training_time/60:.2f} minutes.\")\n",
                "print(f\"Best validation F1: {best_val_f1:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Test Set Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from model import load_trained_model\n",
                "\n",
                "# Load best model\n",
                "best_model, _ = load_trained_model(str(config.output_dir), config.device)\n",
                "\n",
                "# Evaluate on test set\n",
                "test_preds, test_labels, test_loss = evaluate(best_model, test_loader, config.device)\n",
                "test_acc = accuracy_score(test_labels, test_preds)\n",
                "test_f1 = f1_score(test_labels, test_preds, average=\"macro\")\n",
                "\n",
                "print(\"Test Set Results:\")\n",
                "print(f\"  Loss: {test_loss:.4f}\")\n",
                "print(f\"  Accuracy: {test_acc:.4f}\")\n",
                "print(f\"  Macro F1: {test_f1:.4f}\")\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(test_labels, test_preds, digits=4))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Visualize Training History"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Loss plot\n",
                "axes[0].plot(history['train_loss'], label='Train Loss')\n",
                "axes[0].plot(history['val_loss'], label='Val Loss')\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].set_title('Training and Validation Loss')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True)\n",
                "\n",
                "# Metrics plot\n",
                "axes[1].plot(history['val_acc'], label='Val Accuracy')\n",
                "axes[1].plot(history['val_f1'], label='Val F1')\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('Score')\n",
                "axes[1].set_title('Validation Metrics')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from datetime import datetime\n",
                "\n",
                "# Save epoch metrics\n",
                "epoch_df = pd.DataFrame(history)\n",
                "epoch_df['epoch'] = range(1, len(epoch_df) + 1)\n",
                "epoch_csv = config.output_dir / \"epoch_metrics.csv\"\n",
                "epoch_df.to_csv(epoch_csv, index=False)\n",
                "print(f\"Saved epoch metrics to {epoch_csv}\")\n",
                "\n",
                "# Save summary\n",
                "summary = {\n",
                "    'dataset': DATASET_NAME,\n",
                "    'model': config.model_name,\n",
                "    'timestamp': datetime.utcnow().isoformat(),\n",
                "    'best_val_f1': best_val_f1,\n",
                "    'test_loss': test_loss,\n",
                "    'test_acc': test_acc,\n",
                "    'test_f1': test_f1,\n",
                "    'training_minutes': training_time / 60,\n",
                "}\n",
                "\n",
                "summary_df = pd.DataFrame([summary])\n",
                "summary_csv = config.output_dir / \"run_summary.csv\"\n",
                "summary_df.to_csv(summary_csv, index=False)\n",
                "print(f\"Saved run summary to {summary_csv}\")\n",
                "\n",
                "display(summary_df)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}