# ƒê·ªì √°n CS221: VIHATET5: Enhancing Hate Speech Detection in Vietnamese With a Unified Text-to-Text Transformer Model

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Transformers](https://img.shields.io/badge/library-transformers-orange.svg)](https://github.com/huggingface/transformers)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

M·ªôt h·ªá th·ªëng to√†n di·ªán cho b√†i to√°n ph√°t hi·ªán ng√¥n ng·ªØ th√π gh√©t (Hate Speech) v√† b√¨nh lu·∫≠n ƒë·ªôc h·∫°i (Toxic Speech) ti·∫øng Vi·ªát, s·ª≠ d·ª•ng c√°c ki·∫øn tr√∫c SOTA nh∆∞ **PhoBERT/ViSoBERT** v√† **T5/ViT5**.

> üìÑ **Paper**: [ViHATE T5: Enhancing Hate Speech Detection in Vietnamese With a Unified Text-to-Text Transformer Model](https://aclanthology.org/2024.findings-acl.355.pdf) (ACL 2024 Findings)

---

## üìå T·ªïng quan d·ª± √°n

D·ª± √°n cung c·∫•p 3 pipeline ch√≠nh cho ph√©p b·∫°n ƒëi t·ª´ d·ªØ li·ªáu th√¥ ƒë·∫øn m√¥ h√¨nh ho√†n ch·ªânh:
1.  **Pre-training**: Ti·∫øp t·ª•c hu·∫•n luy·ªán T5 v·ªõi c∆° ch·∫ø *Span Corruption* tr√™n d·ªØ li·ªáu ti·∫øng Vi·ªát.
2.  **T5 Fine-tuning**: Hu·∫•n luy·ªán Seq2Seq cho b√†i to√°n ph√¢n lo·∫°i ƒëa t·∫≠p d·ªØ li·ªáu.
3.  **BERT Classification**: Hu·∫•n luy·ªán c√°c m√¥ h√¨nh Encoder-only (PhoBERT, ViSoBERT) truy·ªÅn th·ªëng.

---

## üë• Th√†nh vi√™n nh√≥m

| STT | H·ªç v√† T√™n | MSSV |
| :---: | :--- | :---: |
| 1 | Tr·ªãnh Tr√¢n Tr√¢n | 23521624 |
| 2 | Ph·∫°m Th·ªã Ng·ªçc B√≠ch | 23520148 |
| 3 | Nguy·ªÖn Minh B·∫£o | 23520123 |

---

## üõ† C√†i ƒë·∫∑t & Chu·∫©n b·ªã

### 1. Kh·ªüi t·∫°o m√¥i tr∆∞·ªùng
```bash
# Kh·ªüi t·∫°o venv
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# Windows: .venv\Scripts\activate

# C√†i ƒë·∫∑t th∆∞ vi·ªán
pip install -r requirements.txt
```

### 2. ƒêƒÉng nh·∫≠p HuggingFace (C·∫ßn thi·∫øt ƒë·ªÉ t·∫£i/ƒë·∫©y m√¥ h√¨nh)
```bash
huggingface-cli login
# Ho·∫∑c thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng HF_TOKEN
```

---

## üìä D·ªØ li·ªáu (Datasets)

H·ªá th·ªëng h·ªó tr·ª£ n·∫°p d·ªØ li·ªáu t·ª± ƒë·ªông t·ª´ HuggingFace ho·∫∑c file local:

| T√™n Dataset | Lo·∫°i | M√¥ t·∫£ |
| :--- | :--- | :--- |
| **ViHSD** | Multi-class | 3 nh√£n: CLEAN, OFFENSIVE, HATE |
| **ViCTSD** | Binary | Ph√°t hi·ªán ƒë·ªôc h·∫°i (Toxic/None) |
| **ViHOS** | Hate Spans | Ph√°t hi·ªán v√πng th√π gh√©t |
| **VOZ-HSD** | Binary | D·ªØ li·ªáu l·ªõn (balanced, hate_only, full) |
| **Custom HF** | T√πy ch·ªçn | B·∫•t k·ª≥ dataset n√†o tr√™n HuggingFace (t·ª± nh·∫≠n di·ªán c·ªôt) |

---

## üì¶ C√°c Model & Dataset ƒë√£ hu·∫•n luy·ªán

> **Collection ƒë·∫ßy ƒë·ªß**: T·∫•t c·∫£ c√°c model v√† dataset c·ªßa d·ª± √°n ƒë∆∞·ª£c t·ªïng h·ª£p t·∫°i [CS221 - UIT Collection](https://huggingface.co/collections/Minhbao5xx2/cs221-uit) tr√™n HuggingFace.

D∆∞·ªõi ƒë√¢y l√† c√°c t√†i nguy√™n ch√≠nh ƒë∆∞·ª£c ph√°t tri·ªÉn trong d·ª± √°n n√†y:

*   **Model G√°n nh√£n (Labeling)**: [CS221_Labeling_visobert](https://huggingface.co/Minhbao5xx2/CS221_Labeling_visobert) - Model d·ª±a tr√™n ViSoBERT ƒë∆∞·ª£c d√πng ƒë·ªÉ g√°n nh√£n t·ª± ƒë·ªông cho t·∫≠p d·ªØ li·ªáu l·ªõn.
*   **Dataset ƒë√£ g√°n nh√£n**: [re_VOZ-HSD](https://huggingface.co/datasets/Minhbao5xx2/re_VOZ-HSD) - T·∫≠p d·ªØ li·ªáu VOZ v·ªõi h∆°n 12 tri·ªáu d√≤ng ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† g√°n nh√£n.
*   **Model Fine-tuned (3-datasets - Hate Only)**: [Hate_only_ViT5](https://huggingface.co/Minhbao5xx2/Hate_only_ViT5) - M√¥ h√¨nh ViT5-base ƒë∆∞·ª£c fine-tune ƒë·ªìng th·ªùi tr√™n 3 t·∫≠p d·ªØ li·ªáu (ViHSD, ViCTSD, ViHOS) kh·ªüi t·∫°o t·ª´ checkpoint "hate-only".
*   **Model Fine-tuned (3-datasets - Balanced)**: [balance_Vi_T5](https://huggingface.co/Minhbao5xx2/balance_Vi_T5) - M√¥ h√¨nh ViT5-base ƒë∆∞·ª£c fine-tune ƒë·ªìng th·ªùi tr√™n 3 t·∫≠p d·ªØ li·ªáu kh·ªüi t·∫°o t·ª´ checkpoint "balanced".
*   **Model Fine-tuned (Multi-dataset version)**: [vit5_multi_dataset](https://huggingface.co/Minhbao5xx2/vit5_multi_dataset) - M·ªôt phi√™n b·∫£n kh√°c c·ªßa ViT5-base ƒë∆∞·ª£c hu·∫•n luy·ªán b·∫±ng pipeline `src/train_t5.py`.
*   **Model Pre-trained (Hate Only)**: [pre_train_ViT5_hate_only](https://huggingface.co/Minhbao5xx2/pre_train_ViT5_hate_only) - M√¥ h√¨nh ViT5 ƒë∆∞·ª£c pre-train b·∫±ng c∆° ch·∫ø Span Corruption tr√™n **100,000 m·∫´u** t·ª´ t·∫≠p d·ªØ li·ªáu VOZ "hate-only".
*   **Model Pre-trained (Balanced)**: [balance_pre_train_Vi_T5](https://huggingface.co/Minhbao5xx2/balance_pre_train_Vi_T5) - M√¥ h√¨nh ViT5 ƒë∆∞·ª£c pre-train b·∫±ng c∆° ch·∫ø Span Corruption tr√™n **200,000 m·∫´u** t·ª´ t·∫≠p d·ªØ li·ªáu VOZ "balanced".

---

## ‚öôÔ∏è C·∫•u h√¨nh Model & Training Pipeline

D∆∞·ªõi ƒë√¢y l√† c·∫•u h√¨nh chi ti·∫øt cho t·ª´ng giai ƒëo·∫°n hu·∫•n luy·ªán trong d·ª± √°n:

### 1Ô∏è‚É£ **Giai ƒëo·∫°n Pre-training T5 (Span Corruption)**

**M·ª•c ti√™u**: Ti·∫øp t·ª•c hu·∫•n luy·ªán m√¥ h√¨nh T5 v·ªõi c∆° ch·∫ø Span Corruption tr√™n d·ªØ li·ªáu ti·∫øng Vi·ªát ƒë·ªÉ tƒÉng kh·∫£ nƒÉng hi·ªÉu ng·ªØ c·∫£nh.

**Model Base**: `VietAI/vit5-base` ho·∫∑c `google/mt5-base`

**C·∫•u h√¨nh ch√≠nh**:
```python
# Model & Tokenizer
model_name = "VietAI/vit5-base"
max_length = 256
noise_density = 0.15
mean_noise_span_length = 3.0

# Training Arguments
per_device_train_batch_size = 128  # T√πy GPU
gradient_accumulation_steps = 1
learning_rate = 5e-3
num_train_epochs = 10
warmup_steps = 2000
weight_decay = 0.001
bf16 = True  # B·∫≠t mixed precision cho H200/A100

# Optimizer
optim = "adamw_torch"
gradient_checkpointing = True
```

**Dataset**: 
- `Minhbao5xx2/re_VOZ-HSD` (split: `hate_only` ho·∫∑c `balanced`)
- S·ªë l∆∞·ª£ng samples: 100K (hate-only) ho·∫∑c 200K (balanced)

**Output**: Checkpoint ƒë∆∞·ª£c l∆∞u t·∫°i `vihate_t5_pretrain/` ho·∫∑c `--output_dir` t√πy ch·ªânh.

---

### 2Ô∏è‚É£ **Giai ƒëo·∫°n Fine-tuning T5 (Seq2Seq Classification)**

**M·ª•c ti√™u**: Fine-tune m√¥ h√¨nh T5 (t·ª´ checkpoint pre-trained ho·∫∑c base) tr√™n c√°c t·∫≠p d·ªØ li·ªáu hate speech detection.

**Model Base**: 
- Checkpoint t·ª´ giai ƒëo·∫°n 1: `vihate_t5_pretrain/final`
- Ho·∫∑c tr·ª±c ti·∫øp: `VietAI/vit5-base`

**C·∫•u h√¨nh ch√≠nh**:
```python
# Model & Tokenizer
pre_trained_checkpoint = "vihate_t5_pretrain/final"  # ho·∫∑c "VietAI/vit5-base"
max_length = 256
target_max_length = 10  # ƒê·ªô d√†i label (CLEAN, HATE, OFFENSIVE...)

# Training Arguments
per_device_train_batch_size = 32
per_device_eval_batch_size = 32
gradient_accumulation_steps = 1
learning_rate = 2e-4
num_train_epochs = 4
warmup_ratio = 0.0
weight_decay = 0.01
lr_scheduler_type = "linear"
bf16 = True

# Evaluation
evaluation_strategy = "epoch"
save_strategy = "epoch"
load_best_model_at_end = True
metric_for_best_model = "f1_macro"
```

**Dataset**: 
- `ViHSD`, `ViCTSD`, `ViHOS` (t·ª± ƒë·ªông load t·ª´ HuggingFace)
- Ho·∫∑c t·∫≠p d·ªØ li·ªáu t√πy ch·ªânh

**Output**: Model ƒë∆∞·ª£c l∆∞u t·∫°i `outputs/` ho·∫∑c `--output_dir` t√πy ch·ªânh.

---

### 3Ô∏è‚É£ **Giai ƒëo·∫°n Training BERT-based Models (Classification)**

**M·ª•c ti√™u**: Hu·∫•n luy·ªán c√°c m√¥ h√¨nh encoder-only (PhoBERT, ViSoBERT) cho b√†i to√°n ph√¢n lo·∫°i truy·ªÅn th·ªëng.

**C·∫•u h√¨nh ch√≠nh**:
```python
# Model & Tokenizer
model_name = "uitnlp/visobert"
max_length = 256
num_labels = 3  # T√πy dataset (ViHSD: 3, ViCTSD: 2, ViHOS: 2)

# Training Arguments
per_device_train_batch_size = 16
per_device_eval_batch_size = 32
gradient_accumulation_steps = 1
learning_rate = 2e-5
num_train_epochs = 10
warmup_ratio = 0.1
weight_decay = 0.01
patience = 3  # Early stopping

# Optimizer
optim = "adamw_torch"
```

**Dataset**: 
- `ViHSD`, `ViCTSD`, `ViHOS`
- T·ª± ƒë·ªông x·ª≠ l√Ω label encoding

**Output**: Model ƒë∆∞·ª£c l∆∞u t·∫°i `outputs/` ho·∫∑c `--output_dir` t√πy ch·ªânh.

---

### 4Ô∏è‚É£ **Giai ƒëo·∫°n Auto-Labeling (Optional)**

**M·ª•c ti√™u**: S·ª≠ d·ª•ng m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán ƒë·ªÉ g√°n nh√£n t·ª± ƒë·ªông cho t·∫≠p d·ªØ li·ªáu l·ªõn.

**Model**: `Minhbao5xx2/CS221_Labeling_visobert`

**C·∫•u h√¨nh ch√≠nh**:
```python
# Model & Tokenizer
model_name = "Minhbao5xx2/CS221_Labeling_visobert"
max_length = 256
batch_size = 128 
```

**Dataset Input**: D·ªØ li·ªáu th√¥ (CSV, JSON, Parquet)

**Output**: Dataset ƒë√£ g√°n nh√£n ƒë∆∞·ª£c ƒë·∫©y l√™n HuggingFace Hub.

---

### üìä **So s√°nh c·∫•u h√¨nh gi·ªØa c√°c giai ƒëo·∫°n**

| Giai ƒëo·∫°n | Model Base | Batch Size | Learning Rate | Epochs | Optimizer |
| :--- | :--- | :---: | :---: | :---: | :--- |
| **Pre-training T5** | vit5-base | 128 | 5e-3 | 10 | adamw_torch |
| **Fine-tuning T5** | Pre-trained checkpoint | 32 | 2e-4 | 4 | adamw_torch |
| **Training BERT** | phobert/visobert | 16 | 2e-5 | 10 | adamw_torch |
| **Auto-Labeling** | visobert (fine-tuned) | 128 | - | - | - |

---

## üöÄ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng (Scripts)

### 1. Pre-training T5 (Span Corruption)
```bash
bash scripts/run_pretrain_t5.sh \
    --dataset_name "Minhbao5xx2/re_VOZ-HSD" \
    --split_name "hate_only" \
    --batch_size 128 \
    --epochs 10 \
    --lr 5e-3
```
*L∆∞u √Ω: M·∫∑c ƒë·ªãnh t·ªëi ∆∞u cho H200. V·ªõi GPU nh·ªè, gi·∫£m `batch_size` v√† tƒÉng `gradient_accumulation_steps`.*

### 2. Fine-tuning T5 (Ph√¢n lo·∫°i Seq2Seq)
```bash
bash scripts/run_train_t5.sh \
    --pre_trained_ckpt "vihate_t5_pretrain/final" \
    --batch_size 32 \
    --num_epochs 4 \
    --learning_rate 2e-4 \
    --max_length 256
```

### 3. Hu·∫•n luy·ªán BERT/PhoBERT (Classification)
```bash
bash scripts/run_train_bert.sh \
    --dataset "ViHSD" \
    --model_name "vinai/phobert-base" \
    --epochs 10 \
    --batch_size 16
```

---

## ‚öôÔ∏è Chi ti·∫øt tham s·ªë (CLI Arguments)

### **Script: run_train_t5.sh & run_pretrain_t5.sh**
| Tham s·ªë | M√¥ t·∫£ | T5 Fine-tune | T5 Pre-train |
| :--- | :--- | :--- | :--- |
| `--dataset_name` / `--dataset` | T√™n dataset (HF ho·∫∑c Local) | ‚úÖ | ‚úÖ |
| `--pre_trained_ckpt` | Model g·ªëc (ViT5, checkpoint...) | ‚úÖ | - |
| `--batch_size` | Batch size m·ªói GPU | `32` | `128` |
| `--num_epochs` / `--epochs` | S·ªë epoch hu·∫•n luy·ªán | `4` | `10` |
| `--learning_rate` / `--lr` | T·ªëc ƒë·ªô h·ªçc (Learning Rate) | `2e-4` | `5e-3` |
| `--max_length` | ƒê·ªô d√†i sequence t·ªëi ƒëa | `256` | - |
| `--gradient_accumulation_steps`| T√≠ch l≈©y gradient | `1` | `1` |
| `--weight_decay` | Suy gi·∫£m tr·ªçng s·ªë | `0.01` | `0.001` |
| `--warmup_ratio` / `--warmup_steps`| T·ªâ l·ªá/S·ªë b∆∞·ªõc kh·ªüi ƒë·ªông | `0.0` | `2000` |
| `--seed` | Random seed | `42` | - |
---

## üìä K·∫øt qu·∫£ Auto-Labeling VOZ-HSD Dataset

### Labeling Performance (ViSoBERT Model)

M√¥ h√¨nh **CS221_Labeling_visobert** ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t·ª± ƒë·ªông g√°n nh√£n cho t·∫≠p d·ªØ li·ªáu VOZ-HSD:

| Metric | K·∫øt qu·∫£ |
| :--- | :---: |
| **T·ªïng samples ƒë√£ g√°n nh√£n** | 10,747,733 |
| **Agreement v·ªõi manual labels** | **97.5%** ‚úÖ |
| **Accuracy** | 97.5% |
| **Processing Time** | Batch processing on H200 GPU |

> **Nh·∫≠n x√©t**: M√¥ h√¨nh ViSoBERT ƒë·∫°t ƒë·ªô ch√≠nh x√°c cao **97.5%** so v·ªõi manual labels c·ªßa t√°c gi·∫£ g·ªëc, ch·ª©ng minh t√≠nh hi·ªáu qu·∫£ c·ªßa ph∆∞∆°ng ph√°p auto-labeling. T·∫≠p d·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω ho√†n to√†n v√† s·∫µn s√†ng ƒë·ªÉ s·ª≠ d·ª•ng cho pre-training v√† fine-tuning c√°c m√¥ h√¨nh T5.
---

## üìä K·∫øt qu·∫£ th·ª±c nghi·ªám (Table 3 - Paper)

D∆∞·ªõi ƒë√¢y l√† k·∫øt qu·∫£ chi ti·∫øt tr√™n c√°c t·∫≠p d·ªØ li·ªáu test c·ªßa c√°c m√¥ h√¨nh BERT-based ƒë√£ hu·∫•n luy·ªán:

### K·∫øt qu·∫£ chi ti·∫øt theo Dataset

#### ViHSD Dataset
| Model | Accuracy | Macro F1 |
| :--- | :---: | :---: |
| **ViSoBERT** | 0.8842 | 0.6871 |
| **DistilBERT** (multilingual) | 0.8615 | 0.6224 |
| **BERT** (multilingual, cased) | 0.8665 | 0.6427 |
| **PhoBERT v2** | 0.8725 | 0.6583 |
| **PhoBERT** | 0.8632 | 0.6360 |
| **viBERT** | 0.8596 | 0.6149 |
| **XLM-RoBERTa** | 0.8692 | 0.6544 |
| **BERT** (multilingual, uncased) | 0.8561 | 0.6161 |

#### ViCTSD Dataset
| Model | Accuracy | Macro F1 |
| :--- | :---: | :---: |
| **ViSoBERT** | 0.9035 | 0.7045 |
| **XLM-RoBERTa** | 0.9015 | 0.7153 |
| **PhoBERT v2** | 0.9023 | 0.7139 |
| **PhoBERT** | 0.9078 | 0.7131 |
| **BERT** (multilingual, cased) | 0.8983 | 0.6710 |
| **BERT** (multilingual, uncased) | 0.8993 | 0.6796 |
| **DistilBERT** | 0.8962 | 0.6850 |
| **viBERT** | 0.8881 | 0.6765 |

#### ViHOS Dataset
| Model | Accuracy | Macro F1 |
| :--- | :---: | :---: |
| **ViSoBERT** | 0.9016 | 0.8578 |
| **XLM-RoBERTa** | 0.8834 | 0.8133 |
| **PhoBERT v2** | 0.8492 | 0.7351 |
| **PhoBERT** | 0.8465 | 0.7281 |
| **BERT** (multilingual, cased) | 0.8601 | 0.7637 |
| **BERT** (multilingual, uncased) | 0.8520 | 0.7393 |
| **DistilBERT** | 0.8585 | 0.7615 |
| **viBERT** | 0.8463 | 0.7291 |

### Trung b√¨nh F1 Macro theo Model (across 3 datasets)
| Model | ViHSD F1 | ViCTSD F1 | ViHOS F1 | **Average F1** |
| :--- | :---: | :---: | :---: | :---: |
| **ViSoBERT** | 0.6871 | 0.7045 | 0.8578 | **0.7498** |
| **PhoBERT v2** | 0.6583 | 0.7139 | 0.7351 | **0.7024** |
| **PhoBERT** | 0.6360 | 0.7131 | 0.7281 | **0.6924** |
| **XLM-RoBERTa** | 0.6544 | 0.7153 | 0.8133 | **0.7277** |
| **BERT** (cased) | 0.6427 | 0.6710 | 0.7637 | **0.6925** |
| **BERT** (uncased) | 0.6161 | 0.6796 | 0.7393 | **0.6783** |
| **DistilBERT** | 0.6224 | 0.6850 | 0.7615 | **0.6896** |
| **viBERT** | 0.6149 | 0.6765 | 0.7291 | **0.6735** |
| **Overall Average** | **0.6412** | **0.6949** | **0.7660** | **0.7007** |

---

## üìä K·∫øt qu·∫£ T5 Fine-tuning (Table 4 - Paper)

D∆∞·ªõi ƒë√¢y l√† k·∫øt qu·∫£ chi ti·∫øt c·ªßa c√°c m√¥ h√¨nh T5 ƒë∆∞·ª£c fine-tune tr√™n 3 t·∫≠p d·ªØ li·ªáu:

### K·∫øt qu·∫£ chi ti·∫øt theo Dataset

#### T5 Models Results
| Model | Dataset | Accuracy | F1 Weighted | F1 Macro |
| :--- | :--- | :---: | :---: | :---: |
| **ViT5 (Base)** | ViHSD | 0.8777 | 0.8787 | 0.6625 |
| **ViT5 (Base)** | ViCTSD | 0.9080 | 0.9178 | 0.7163 |
| **ViT5 (Base)** | ViHOS | 0.9075 | 0.9000 | 0.8612 |
| **mT5 (Base)** | ViHSD | 0.8746 | 0.8877 | 0.6246 |
| **mT5 (Base)** | ViCTSD | 0.8932 | 0.9024 | 0.7053 |
| **mT5 (Base)** | ViHOS | 0.9075 | 0.8957 | 0.8501 |
| **ViHateT5 (Ours)** | ViHSD | **0.8815** | **0.8849** | **0.6698** |
| **ViHateT5 (Ours)** | ViCTSD | **0.9105** | **0.9158** | **0.7189** |
| **ViHateT5 (Ours)** | ViHOS | **0.9081** | **0.9055** | **0.8616** |

### Trung b√¨nh F1 Macro theo Model T5 (across 3 datasets)
| Model | ViHSD F1 | ViCTSD F1 | ViHOS F1 | **Average F1** |
| :--- | :---: | :---: | :---: | :---: |
| **ViHateT5 (Ours)** | **0.6698** | **0.7189** | **0.8616** | **0.7501** ‚≠ê |
| **ViT5 (Base)** | 0.6625 | 0.7163 | 0.8612 | 0.7467 |
| **mT5 (Base)** | 0.6246 | 0.7053 | 0.8501 | 0.7267 |

---

## üìä K·∫øt qu·∫£ ViHateT5 Pre-trained Impact (Table 5 - Paper)

D∆∞·ªõi ƒë√¢y l√† k·∫øt qu·∫£ ·∫£nh h∆∞·ªüng c·ªßa pre-training v·ªõi c√°c t·ªâ l·ªá d·ªØ li·ªáu kh√°c nhau tr√™n hi·ªáu su·∫•t c·ªßa ViHateT5:

### Pre-trained tr√™n 100K samples (Hate-Only)
| Dataset | Accuracy | F1 Weighted | F1 Macro |
| :--- | :---: | :---: | :---: |
| **ViHSD** | 0.8789 | 0.8784 | 0.6808 |
| **ViCTSD** | 0.9070 | 0.9283 | 0.6586 |
| **ViHOS** | 0.9039 | 0.8981 | 0.8541 |

### Pre-trained tr√™n 200K samples (Balanced)
| Dataset | Accuracy | F1 Weighted | F1 Macro |
| :--- | :---: | :---: | :---: |
| **ViHSD** | 0.8815 | 0.8849 | 0.6698 |
| **ViCTSD** | 0.9105 | 0.9158 | 0.7189 |
| **ViHOS** | 0.9081 | 0.9055 | 0.8616 |

### Trung b√¨nh F1 Macro theo Pre-training Checkpoint
| Pre-training Setup | ViHSD F1 | ViCTSD F1 | ViHOS F1 | **Average F1** |
| :--- | :---: | :---: | :---: | :---: |
| **ViHateT5 (Ours) - Pre-trained (200K, Balanced)** | **0.6698** | **0.7189** | **0.8616** | **0.7501** ‚≠ê |
| **Pre-trained (100K, Hate-Only)** | 0.6808 | 0.6586 | 0.8541 | 0.7312 |

---

## **Table 6 ‚Äî BERT-based models comparison (sorted by Macro F1)**

### Multilingual Pre-trained Models
| Model | Accuracy | F1 Weighted | F1 Macro |
| :--- | :---: | :---: | :---: |
| xlm-roberta-base | 0.9189 | 0.7722 | 0.8028 |
| xlm-roberta-large | 0.9204 | 0.7755 | 0.7968 |
| google-bert/bert-base-multilingual-uncased | 0.9102 | 0.7557 | 0.7784 |
| distilbert-base-multilingual-cased | 0.9115 | 0.7459 | 0.7754 |
| google-bert/bert-base-multilingual-cased | 0.9094 | 0.7548 | 0.7740 |

### Monolingual Pre-trained Models
| Model | Accuracy | F1 Weighted | F1 Macro |
| :--- | :---: | :---: | :---: |
| **uitnlp/visobert** | 0.9296 | 0.8051 | **0.8128** |
| vinai/phobert-base-v2 | 0.9216 | 0.7888 | 0.7810 |
| FPTAI/vibert-base-cased | 0.9117 | 0.7385 | 0.7771 |
| vinai/phobert-base | 0.9231 | 0.7562 | 0.7764 |
| vinai/phobert-large | 0.9245 | 0.7895 | 0.7832 |
---

## üìà K·∫øt qu·∫£ & Output

Sau khi ch·∫°y training, k·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c `outputs/` ho·∫∑c `vihate_t5_pretrain/`:

-   **Model Checkpoints**: File tr·ªçng s·ªë (`.bin` / `.safetensors`) v√† c·∫•u h√¨nh.
-   **`run_summary.csv`**: T·ªïng h·ª£p k·∫øt qu·∫£ t·ªët nh·∫•t (F1, Accuracy, Loss).
-   **`epoch_metrics.csv`**: Chi ti·∫øt c√°c ch·ªâ s·ªë qua t·ª´ng epoch.
-   **`results/evaluation_results.csv`**: K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n c√°c t·∫≠p test ri√™ng bi·ªát.

---

## üí° T·ªëi ∆∞u h√≥a hi·ªáu nƒÉng (Hardware Tips)

> **L∆∞u √Ω**: T·∫•t c·∫£ c√°c k·∫øt qu·∫£ th·ª±c nghi·ªám trong d·ª± √°n n√†y ƒë·ªÅu ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n GPU **NVIDIA H200** (ƒë∆∞·ª£c cung c·∫•p b·ªüi FPT th√¥ng qua voucher) v√† **P100**.

T√πy v√†o c·∫•u h√¨nh ph·∫ßn c·ª©ng, b·∫°n n√™n ƒëi·ªÅu ch·ªânh c√°c tham s·ªë sau ƒë·ªÉ ƒë·∫°t t·ªëc ƒë·ªô cao nh·∫•t:

-   **GPU H200 (141GB)**: C√≥ th·ªÉ d√πng `batch_size=128` cho pre-training.
-   **GPU A100/A800 / P100**: Khuy·∫øn ngh·ªã `batch_size=128-256`.
-   **GPU Ph·ªï th√¥ng (8GB-16GB)**: 
    -   B·∫≠t `gradient_checkpointing=True`.
    -   S·ª≠ d·ª•ng `gradient_accumulation_steps` (v√≠ d·ª•: 8 ho·∫∑c 16) ƒë·ªÉ b√π ƒë·∫Øp batch size nh·ªè.
    -   Gi·∫£m `max_length` xu·ªëng 128 ho·∫∑c 256.

---

## üìö Citation

N·∫øu b·∫°n s·ª≠ d·ª•ng code, dataset ho·∫∑c model trong nghi√™n c·ª©u, vui l√≤ng cite paper sau:

```bibtex
@inproceedings{nguyen2024vihate,
  title={ViHATE T5: Enhancing Hate Speech Detection in Vietnamese With a Unified Text-to-Text Transformer Model},
  author={Nguyen, Luan Thanh},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2024},
  pages={5948--5961},
  year={2024},
  url={https://aclanthology.org/2024.findings-acl.355.pdf}
}
```

**Paper**: [ViHATE T5: Enhancing Hate Speech Detection in Vietnamese With a Unified Text-to-Text Transformer Model](https://aclanthology.org/2024.findings-acl.355.pdf) (ACL 2024 Findings)

---

¬© 2024 Vietnamese Hate Speech Team. D·ª± √°n ph·ª•c v·ª• m·ª•c ƒë√≠ch nghi√™n c·ª©u.






