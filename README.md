# ğŸ‡»ğŸ‡³ Vietnamese Hate Speech Detection Pipeline

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Transformers](https://img.shields.io/badge/library-transformers-orange.svg)](https://github.com/huggingface/transformers)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Má»™t há»‡ thá»‘ng toÃ n diá»‡n cho bÃ i toÃ¡n phÃ¡t hiá»‡n ngÃ´n ngá»¯ thÃ¹ ghÃ©t (Hate Speech) vÃ  bÃ¬nh luáº­n Ä‘á»™c háº¡i (Toxic Speech) tiáº¿ng Viá»‡t, sá»­ dá»¥ng cÃ¡c kiáº¿n trÃºc SOTA nhÆ° **PhoBERT/ViSoBERT** vÃ  **T5/ViT5**.

---

## ğŸ“Œ Tá»•ng quan dá»± Ã¡n

Dá»± Ã¡n cung cáº¥p 3 pipeline chÃ­nh cho phÃ©p báº¡n Ä‘i tá»« dá»¯ liá»‡u thÃ´ Ä‘áº¿n mÃ´ hÃ¬nh hoÃ n chá»‰nh:
1.  **Pre-training**: Tiáº¿p tá»¥c huáº¥n luyá»‡n T5 vá»›i cÆ¡ cháº¿ *Span Corruption* trÃªn dá»¯ liá»‡u tiáº¿ng Viá»‡t.
2.  **T5 Fine-tuning**: Huáº¥n luyá»‡n Seq2Seq cho bÃ i toÃ¡n phÃ¢n loáº¡i Ä‘a táº­p dá»¯ liá»‡u.
3.  **BERT Classification**: Huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh Encoder-only (PhoBERT, ViSoBERT) truyá»n thá»‘ng.

---

## ğŸ›  CÃ i Ä‘áº·t & Chuáº©n bá»‹

### 1. Khá»Ÿi táº¡o mÃ´i trÆ°á»ng
```bash
# Khá»Ÿi táº¡o venv
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# Windows: .venv\Scripts\activate

# CÃ i Ä‘áº·t thÆ° viá»‡n
pip install -r requirements.txt
```

### 2. ÄÄƒng nháº­p HuggingFace (Cáº§n thiáº¿t Ä‘á»ƒ táº£i/Ä‘áº©y mÃ´ hÃ¬nh)
```bash
huggingface-cli login
# Hoáº·c thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng HF_TOKEN
```

---

## ğŸ“Š Dá»¯ liá»‡u (Datasets)

Há»‡ thá»‘ng há»— trá»£ náº¡p dá»¯ liá»‡u tá»± Ä‘á»™ng tá»« HuggingFace hoáº·c file local:

| TÃªn Dataset | Loáº¡i | MÃ´ táº£ |
| :--- | :--- | :--- |
| **ViHSD** | Multi-class | 3 nhÃ£n: CLEAN, OFFENSIVE, HATE |
| **ViCTSD** | Binary | PhÃ¡t hiá»‡n Ä‘á»™c háº¡i (Toxic/None) |
| **ViHOS** | Hate Spans | PhÃ¡t hiá»‡n vÃ¹ng thÃ¹ ghÃ©t |
| **VOZ-HSD** | Binary | Dá»¯ liá»‡u lá»›n (balanced, hate_only, full) |
| **Custom HF** | TÃ¹y chá»n | Báº¥t ká»³ dataset nÃ o trÃªn HuggingFace (tá»± nháº­n diá»‡n cá»™t) |

---

## ğŸ“¦ CÃ¡c Model & Dataset Ä‘Ã£ huáº¥n luyá»‡n

> **Collection Ä‘áº§y Ä‘á»§**: Táº¥t cáº£ cÃ¡c model vÃ  dataset cá»§a dá»± Ã¡n Ä‘Æ°á»£c tá»•ng há»£p táº¡i [CS221 - UIT Collection](https://huggingface.co/collections/Minhbao5xx2/cs221-uit) trÃªn HuggingFace.

DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c tÃ i nguyÃªn chÃ­nh Ä‘Æ°á»£c phÃ¡t triá»ƒn trong dá»± Ã¡n nÃ y:

*   **Model GÃ¡n nhÃ£n (Labeling)**: [CS221_Labeling_visobert](https://huggingface.co/Minhbao5xx2/CS221_Labeling_visobert) - Model dá»±a trÃªn ViSoBERT Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ gÃ¡n nhÃ£n tá»± Ä‘á»™ng cho táº­p dá»¯ liá»‡u lá»›n.
*   **Dataset Ä‘Ã£ gÃ¡n nhÃ£n**: [re_VOZ-HSD](https://huggingface.co/datasets/Minhbao5xx2/re_VOZ-HSD) - Táº­p dá»¯ liá»‡u VOZ vá»›i hÆ¡n 12 triá»‡u dÃ²ng Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ vÃ  gÃ¡n nhÃ£n.
*   **Model Fine-tuned (3-datasets - Hate Only)**: [Hate_only_ViT5](https://huggingface.co/Minhbao5xx2/Hate_only_ViT5) - MÃ´ hÃ¬nh ViT5-base Ä‘Æ°á»£c fine-tune Ä‘á»“ng thá»i trÃªn 3 táº­p dá»¯ liá»‡u (ViHSD, ViCTSD, ViHOS) khá»Ÿi táº¡o tá»« checkpoint "hate-only".
*   **Model Fine-tuned (3-datasets - Balanced)**: [balance_Vi_T5](https://huggingface.co/Minhbao5xx2/balance_Vi_T5) - MÃ´ hÃ¬nh ViT5-base Ä‘Æ°á»£c fine-tune Ä‘á»“ng thá»i trÃªn 3 táº­p dá»¯ liá»‡u khá»Ÿi táº¡o tá»« checkpoint "balanced".
*   **Model Fine-tuned (Multi-dataset version)**: [vit5_multi_dataset](https://huggingface.co/Minhbao5xx2/vit5_multi_dataset) - Má»™t phiÃªn báº£n khÃ¡c cá»§a ViT5-base Ä‘Æ°á»£c huáº¥n luyá»‡n báº±ng pipeline `src/train_t5.py`.
*   **Model Pre-trained (Hate Only)**: [pre_train_ViT5_hate_only](https://huggingface.co/Minhbao5xx2/pre_train_ViT5_hate_only) - MÃ´ hÃ¬nh ViT5 Ä‘Æ°á»£c pre-train báº±ng cÆ¡ cháº¿ Span Corruption trÃªn **100,000 máº«u** tá»« táº­p dá»¯ liá»‡u VOZ "hate-only".
*   **Model Pre-trained (Balanced)**: [balance_pre_train_Vi_T5](https://huggingface.co/Minhbao5xx2/balance_pre_train_Vi_T5) - MÃ´ hÃ¬nh ViT5 Ä‘Æ°á»£c pre-train báº±ng cÆ¡ cháº¿ Span Corruption trÃªn **200,000 máº«u** tá»« táº­p dá»¯ liá»‡u VOZ "balanced".

---

## ğŸš€ HÆ°á»›ng dáº«n sá»­ dá»¥ng (Scripts)

### 1. Pre-training T5 (Span Corruption)
```bash
bash scripts/run_pretrain_t5.sh \
    --dataset_name "Minhbao5xx2/re_VOZ-HSD" \
    --split_name "hate_only" \
    --batch_size 512 \
    --epochs 10 \
    --lr 5e-3
```
*LÆ°u Ã½: Máº·c Ä‘á»‹nh tá»‘i Æ°u cho H200. Vá»›i GPU nhá», giáº£m `batch_size` vÃ  tÄƒng `gradient_accumulation_steps`.*

### 2. Fine-tuning T5 (PhÃ¢n loáº¡i Seq2Seq)
```bash
bash scripts/run_train_t5.sh \
    --pre_trained_ckpt "vihate_t5_pretrain/final" \
    --batch_size 32 \
    --num_epochs 4 \
    --learning_rate 2e-4 \
    --max_length 256
```

### 3. Huáº¥n luyá»‡n BERT/PhoBERT (Classification)
```bash
bash scripts/run_train_bert.sh \
    --dataset "ViHSD" \
    --model_name "vinai/phobert-base" \
    --epochs 10 \
    --batch_size 16
```

---

## âš™ï¸ Chi tiáº¿t tham sá»‘ (CLI Arguments)

### **Script: run_train_t5.sh & run_pretrain_t5.sh**
| Tham sá»‘ | MÃ´ táº£ | T5 Fine-tune | T5 Pre-train |
| :--- | :--- | :--- | :--- |
| `--dataset_name` / `--dataset` | TÃªn dataset (HF hoáº·c Local) | âœ… | âœ… |
| `--pre_trained_ckpt` | Model gá»‘c (ViT5, checkpoint...) | âœ… | - |
| `--batch_size` | Batch size má»—i GPU | `32` | `512` |
| `--num_epochs` / `--epochs` | Sá»‘ epoch huáº¥n luyá»‡n | `4` | `10` |
| `--learning_rate` / `--lr` | Tá»‘c Ä‘á»™ há»c (Learning Rate) | `2e-4` | `5e-3` |
| `--max_length` | Äá»™ dÃ i sequence tá»‘i Ä‘a | `256` | - |
| `--gradient_accumulation_steps`| TÃ­ch lÅ©y gradient | `1` | `1` |
| `--weight_decay` | Suy giáº£m trá»ng sá»‘ | `0.01` | `0.001` |
| `--warmup_ratio` / `--warmup_steps`| Tá»‰ lá»‡/Sá»‘ bÆ°á»›c khá»Ÿi Ä‘á»™ng | `0.0` | `2000` |
| `--seed` | Random seed | `42` | - |

---

## ğŸ“Š Káº¿t quáº£ thá»±c nghiá»‡m (Table 3 - Paper)

DÆ°á»›i Ä‘Ã¢y lÃ  káº¿t quáº£ chi tiáº¿t trÃªn cÃ¡c táº­p dá»¯ liá»‡u test cá»§a cÃ¡c mÃ´ hÃ¬nh BERT-based Ä‘Ã£ huáº¥n luyá»‡n:

### Káº¿t quáº£ chi tiáº¿t theo Dataset

#### ViHSD Dataset
| Model | Accuracy | Macro F1 |
| :--- | :---: | :---: |
| **ViSoBERT** | 0.8842 | 0.6871 |
| **DistilBERT** (multilingual) | 0.8615 | 0.6224 |
| **BERT** (multilingual, cased) | 0.8665 | 0.6427 |
| **PhoBERT v2** | 0.8725 | 0.6583 |
| **PhoBERT** | 0.8632 | 0.6360 |
| **viBERT** | 0.8596 | 0.6149 |
| **XLM-RoBERTa** | 0.8692 | 0.6544 |
| **BERT** (multilingual, uncased) | 0.8561 | 0.6161 |

#### ViCTSD Dataset
| Model | Accuracy | Macro F1 |
| :--- | :---: | :---: |
| **BERT** (multilingual, cased) | 0.8800 | 0.6886 |
| **BERT** (multilingual, uncased) | 0.8820 | 0.6569 |
| **DistilBERT** | 0.8640 | 0.6634 |
| **XLM-RoBERTa** | 0.8990 | 0.7231 |
| **PhoBERT** | 0.8750 | 0.7210 |
| **PhoBERT v2** | 0.8890 | 0.7304 |
| **viBERT** | 0.8920 | 0.6946 |
| **ViSoBERT** | 0.9050 | 0.7483 |

#### ViHOS Dataset
| Model | Accuracy | Macro F1 |
| :--- | :---: | :---: |
| **ViSoBERT** | 0.9231 | 0.9230 |
| **viBERT** | 0.8590 | 0.8589 |
| **BERT** (multilingual, uncased) | 0.8707 | 0.8706 |
| **BERT** (multilingual, cased) | 0.8834 | 0.8832 |
| **XLM-RoBERTa** | 0.8879 | 0.8878 |
| **PhoBERT v2** | 0.9033 | 0.9031 |
| **PhoBERT** | 0.8906 | 0.8903 |
| **DistilBERT** | 0.8707 | 0.8706 |

### Trung bÃ¬nh F1 Macro theo Model (across 3 datasets)
| Model | ViHSD F1 | ViCTSD F1 | ViHOS F1 | **Average F1** |
| :--- | :---: | :---: | :---: | :---: |
| **ViSoBERT** | 0.6871 | 0.7483 | 0.9230 | **0.7861** |
| **PhoBERT v2** | 0.6583 | 0.7304 | 0.9031 | **0.7639** |
| **PhoBERT** | 0.6360 | 0.7210 | 0.8903 | **0.7491** |
| **XLM-RoBERTa** | 0.6544 | 0.7231 | 0.8878 | **0.7551** |
| **BERT** (cased) | 0.6427 | 0.6886 | 0.8832 | **0.7382** |
| **viBERT** | 0.6149 | 0.6946 | 0.8589 | **0.7228** |
| **BERT** (uncased) | 0.6161 | 0.6569 | 0.8706 | **0.7145** |
| **DistilBERT** | 0.6224 | 0.6634 | 0.8706 | **0.7188** |
| **Overall Average** | **0.6412** | **0.7033** | **0.8911** | **0.7452** |

---

## ğŸ“Š Káº¿t quáº£ T5 Fine-tuning (Table 4 - Paper)

DÆ°á»›i Ä‘Ã¢y lÃ  káº¿t quáº£ chi tiáº¿t cá»§a cÃ¡c mÃ´ hÃ¬nh T5 Ä‘Æ°á»£c fine-tune trÃªn 3 táº­p dá»¯ liá»‡u:

### Káº¿t quáº£ chi tiáº¿t theo Dataset

#### T5 Models Results
| Model | Dataset | Accuracy | F1 Weighted | F1 Macro |
| :--- | :--- | :---: | :---: | :---: |
| **ViT5 (Base)** | ViHSD | 0.8777 | 0.8787 | 0.6625 |
| **ViT5 (Base)** | ViCTSD | 0.9080 | 0.9178 | 0.7163 |
| **ViT5 (Base)** | ViHOS | 0.9075 | 0.9000 | 0.8612 |
| **mT5 (Base)** | ViHSD | 0.8746 | 0.8877 | 0.6246 |
| **mT5 (Base)** | ViCTSD | 0.8932 | 0.9024 | 0.7053 |
| **mT5 (Base)** | ViHOS | 0.9075 | 0.8957 | 0.8501 |
| **ViHateT5** | ViHSD | 0.8876 | 0.8914 | 0.6867 |
| **ViHateT5** | ViCTSD | 0.9178 | 0.9080 | 0.7163 |
| **ViHateT5** | ViHOS | 0.9020 | 0.9100 | 0.8637 |

### Trung bÃ¬nh F1 Macro theo Model T5 (across 3 datasets)
| Model | ViHSD F1 | ViCTSD F1 | ViHOS F1 | **Average F1** |
| :--- | :---: | :---: | :---: | :---: |
| **ViHateT5** | 0.6867 | 0.7163 | 0.8637 | **0.7556** |
| **ViT5 (Base)** | 0.6625 | 0.7163 | 0.8612 | **0.7467** |
| **mT5 (Base)** | 0.6246 | 0.7053 | 0.8501 | **0.7267** |
| **Overall Average** | **0.6579** | **0.7126** | **0.8583** | **0.7430** |

---

## ğŸ“Š Káº¿t quáº£ ViHateT5 Pre-trained Impact (Table 5 - Paper)

DÆ°á»›i Ä‘Ã¢y lÃ  káº¿t quáº£ áº£nh hÆ°á»Ÿng cá»§a pre-training vá»›i cÃ¡c tá»‰ lá»‡ dá»¯ liá»‡u khÃ¡c nhau trÃªn hiá»‡u suáº¥t cá»§a ViHateT5:

### Pre-trained trÃªn 100K samples (Hate-Only)
| Dataset | Accuracy | F1 Weighted | F1 Macro |
| :--- | :---: | :---: | :---: |
| **ViHSD** | 0.8789 | 0.8784 | 0.6808 |
| **ViCTSD** | 0.9070 | 0.9283 | 0.6586 |
| **ViHOS** | 0.9039 | 0.8981 | 0.8541 |

### Pre-trained trÃªn 200K samples (Balanced)
| Dataset | Accuracy | F1 Weighted | F1 Macro |
| :--- | :---: | :---: | :---: |
| **ViHSD** | 0.8843 | 0.8919 | 0.6621 |
| **ViCTSD** | 0.8630 | 0.8550 | 0.6921 |
| **ViHOS** | 0.9103 | 0.9027 | 0.8598 |

### Trung bÃ¬nh F1 Macro theo Pre-training Checkpoint
| Pre-training Setup | ViHSD F1 | ViCTSD F1 | ViHOS F1 | **Average F1** |
| :--- | :---: | :---: | :---: | :---: |
| **Pre-trained (100K, Hate-Only)** | 0.6808 | 0.6586 | 0.8541 | **0.7312** |
| **Pre-trained (200K, Balanced)** | 0.6621 | 0.6921 | 0.8598 | **0.7380** |
| **Fine-tuned from scratch (ViHateT5)** | 0.6867 | 0.7163 | 0.8637 | **0.7556** |
| **Overall Average** | **0.6765** | **0.6890** | **0.8592** | **0.7416** |

---

## ğŸ“ˆ Káº¿t quáº£ & Output

Sau khi cháº¡y training, káº¿t quáº£ sáº½ Ä‘Æ°á»£c lÆ°u vÃ o thÆ° má»¥c `outputs/` hoáº·c `vihate_t5_pretrain/`:

-   **Model Checkpoints**: File trá»ng sá»‘ (`.bin` / `.safetensors`) vÃ  cáº¥u hÃ¬nh.
-   **`run_summary.csv`**: Tá»•ng há»£p káº¿t quáº£ tá»‘t nháº¥t (F1, Accuracy, Loss).
-   **`epoch_metrics.csv`**: Chi tiáº¿t cÃ¡c chá»‰ sá»‘ qua tá»«ng epoch.
-   **`results/evaluation_results.csv`**: Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn cÃ¡c táº­p test riÃªng biá»‡t.

---

## ğŸ’¡ Tá»‘i Æ°u hÃ³a hiá»‡u nÄƒng (Hardware Tips)

> **LÆ°u Ã½**: Táº¥t cáº£ cÃ¡c káº¿t quáº£ thá»±c nghiá»‡m trong dá»± Ã¡n nÃ y Ä‘á»u Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn GPU **NVIDIA H200** vÃ  **P100**.

TÃ¹y vÃ o cáº¥u hÃ¬nh pháº§n cá»©ng, báº¡n nÃªn Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ sau Ä‘á»ƒ Ä‘áº¡t tá»‘c Ä‘á»™ cao nháº¥t:

-   **GPU H200 (141GB)**: CÃ³ thá»ƒ dÃ¹ng `batch_size=512` cho pre-training.
-   **GPU A100/A800 / P100**: Khuyáº¿n nghá»‹ `batch_size=128-256`.
-   **GPU Phá»• thÃ´ng (8GB-16GB)**: 
    -   Báº­t `gradient_checkpointing=True`.
    -   Sá»­ dá»¥ng `gradient_accumulation_steps` (vÃ­ dá»¥: 8 hoáº·c 16) Ä‘á»ƒ bÃ¹ Ä‘áº¯p batch size nhá».
    -   Giáº£m `max_length` xuá»‘ng 128 hoáº·c 256.

---

## ğŸ“‚ Cáº¥u trÃºc thÆ° má»¥c

```text
.
â”œâ”€â”€ src/                    # MÃ£ nguá»“n chÃ­nh (Python)
â”œâ”€â”€ scripts/                # Bash scripts cháº¡y nhanh
â”œâ”€â”€ outputs/                # LÆ°u trá»¯ model checkpoints
â”œâ”€â”€ results/                # LÆ°u trá»¯ káº¿t quáº£ Ä‘Ã¡nh giÃ¡ (CSV)
â””â”€â”€ requirements.txt        # Danh sÃ¡ch thÆ° viá»‡n cáº§n thiáº¿t
```

---

## âš ï¸ Giáº£i quyáº¿t sá»± cá»‘ thÆ°á»ng gáº·p

1.  **Lá»—i OOM**: Giáº£m `batch_size`, tÄƒng `gradient_accumulation_steps`, hoáº·c giáº£m `max_length`.
2.  **ModuleNotFoundError**: `pip install -r requirements.txt` vÃ  cháº¡y tá»« thÆ° má»¥c gá»‘c.
3.  **Tá»‘c Ä‘á»™ cháº­m**: Kiá»ƒm tra `dataloader_num_workers` vÃ  sá»­ dá»¥ng GPU phÃ¹ há»£p.

---
Â© 2024 Vietnamese Hate Speech Team. Dá»± Ã¡n phá»¥c vá»¥ má»¥c Ä‘Ã­ch nghiÃªn cá»©u.
